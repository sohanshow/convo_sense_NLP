# -*- coding: utf-8 -*-
"""V3_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18eBSwMR2r6NAB-3xXk68v2duph09OPrQ
"""

from nltk.stem import WordNetLemmatizer
import nltk
# import operator

import numpy as np

nltk.download('omw-1.4')
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')

from google.colab import drive
drive.mount('/content/drive')

# Frequency to determine the uniqueness of word (default 100000 from my experience , can change by checking skimming the wordlist)
# (In next steps we can set different bars for native , converssational and other speakers)
bar = 100000

# Number of lineas already read
nlines= 0

# input txt file path
filepath = "/content/drive/MyDrive/Colab Notebooks/V2/input.txt"

# wordlist file path
wordlist_path= "/content/drive/MyDrive/Colab Notebooks/V2/google_freq_dump.json"

import json

with open(wordlist_path) as json_file:
    word_freq_allwords = json.load(json_file)

input_freq = dict()
unique = set()
oov=set()

def get_words(text):
  new_unique= []
  sorted_unique=[]

  for line in text.split("\n"):
    token = tokenizer.tokenize(text)
    token = [x.strip().lower() for x in token]
    for x in token:
      if not x.isalpha():
        continue
      t = lemmatizer.lemmatize(x)

      if t in word_freq_allwords:
        if word_freq_allwords[t] < bar:
          if t not in input_freq:
            input_freq[x] = 1
          else:
            input_freq[x] = input_freq[x] + 1
          unique.add(x)
          new_unique.append(x)
      else:
        oov.add(x)

  if len(new_unique)>1:
    for u in new_unique:
      sorted_unique.append((u,input_freq[u]))
    sorted_unique = sorted(sorted_unique,key=lambda t: t[1],reverse = False)
    new_unique = [u[0] for u in sorted_unique]


  return list(set(new_unique))

text = '''ID2299 is an elliptical galaxy 9 billion light-years away. It was found and scurvy detailed in January 2021, due to scurvy its phenomenon of catastrophic gas loss. This is due, Fouriertransform was a good guy tho. Yeah I think. catastrophic unless qubit the prolonged observations are inexplicably misleading or a poorly understood mechanism is at hand, to a catastrophic merger – prompting a secondary part of inexplicably the galaxy that hosts rapid star formation. ID2299's high star formation inexplicably rate is far outweighed by its ejection of gas. Its trailing tail has grown qubit to approximately half of its size. ID2299 is extrapolated to lose so much more gas pernicious scurvy that it will only remain active – capable of qubit new star pernicious formation – for a few more scurvy tens of millions of years.'''

new_unique = get_words(text)
new_unique

unique

input_freq

text = "2021 20222 2023 2021 trichotillomania"

new_unique = get_words(text)
new_unique

unique

oov

input_freq